{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SOLVING AN OPTIMIZATION PROBLEM WITHOUT CONSTRAINTS\n",
    "- Fixed-pitch gradient method "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Algorithm 9.3 - Gradient descent method  \n",
    "- Given a starting point $ x_{0} âˆˆ domf $.\n",
    "- Repeat  \n",
    "  1. Search direction.$ \\Delta x:=-\\nabla f(x) $  \n",
    "  2. Line search. Choose step size $t$ via exact or backtracking line search.\n",
    "  3. Update.  $ x:=x+t \\Delta x $  \n",
    "- Until stopping criterion is satisfied.(e.x.$ \\|\\nabla f(x)\\|_{2} \\leq \\eta $)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import modular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy .linalg import norm \n",
    "from time import process_time \n",
    "from Visualg import Visualg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization of variables "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. $ k < 10000$\n",
    "2. $ t_0 = \\alpha_{0}$\n",
    "3. $ \\eta = 0.000001 $\n",
    "4. $ \\|\\nabla f(x)\\|_{2} $\n",
    "5. $ t $\n",
    "6. what is the criteria ? the value of ojb function $f(x^k)$ \n",
    "7. $x = x_{0}$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'alpha0' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-6f9cb29161c1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0miter_max\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10000\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mgradient_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0malpha0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mthreshold\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.000001\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mgradient_norm_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'alpha0' is not defined"
     ]
    }
   ],
   "source": [
    "iter_max = 10000 \n",
    "gradient_step = alpha0 \n",
    "threshold = 0.000001\n",
    "\n",
    "gradient_norm_list = [] \n",
    "gradient_step_list = [] \n",
    "criteria_list = [] \n",
    "time_start = process_time () \n",
    "x = x0 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loop on iterations  \n",
    "- repeat:  \n",
    "  1. Search direction.$ \\Delta x:=-\\nabla f(x) $  \n",
    "  2. Line search. Choose step size $t$ via exact or backtracking line search.\n",
    "  3. Update.  \n",
    "    $ x:=x+t \\Delta x $   \n",
    "    $  (x^{k+1}:=x^{k}+t \\Delta x^{k} )$  \n",
    "- until stopping criterion is satisfied:  \n",
    "  Compute $ \\|\\nabla f(x)\\|_{2} $  \n",
    "  Check $ \\|\\nabla f(x)\\|_{2} \\leq \\eta $  \n",
    "- store the values of variables:  \n",
    "  $f(x) = [f(x^0), f(x^1), ...]$  \n",
    "  $t = [t_0,t_1,....]$  \n",
    "  $  \\|\\nabla f(x)\\|_{2}  = [ \\|\\nabla f(x_0)\\|_{2},  \\|\\nabla f(x_1)\\|_{2},....  ]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range (iter_max): \n",
    "        \n",
    "    # Value of the criterion and the gradient \n",
    "    criterion, gradient = Oracle (x) \n",
    "\n",
    "    # Convergence test \n",
    "    gradient_norm = norm (gradient) \n",
    "    if gradient_norm <= threshold: \n",
    "        break \n",
    "\n",
    "    # Direction of descent \n",
    "    direction = -gradient\n",
    "        \n",
    "    # Update of variables \n",
    "    x = x + (gradient_step * direction) \n",
    "        \n",
    "    # Evolution of gradient, step, and criterion \n",
    "    gradient_norm_list.append (gradient_norm) \n",
    "    gradient_step_list.append (gradient_step) \n",
    "    criteria_list.append (criterion) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Gradient_F (Oracle, x0, alpha0): \n",
    "    \n",
    "    from numpy .linalg import norm \n",
    "    from time import process_time \n",
    "    \n",
    "    from Visualg import Visualg\n",
    "    \n",
    "    ##### Initialization of variables \n",
    "    \n",
    "    iter_max = 10000 \n",
    "    gradient_step = alpha0 \n",
    "    threshold = 0.000001 \n",
    "    \n",
    "    gradient_norm_list = [] \n",
    "    gradient_step_list = [] \n",
    "    criteria_list = [] \n",
    "\n",
    "    time_start = process_time () \n",
    "    \n",
    "    x = x0 \n",
    "\n",
    "    ##### Loop on iterations \n",
    "\n",
    "    for k in range (iter_max): \n",
    "        \n",
    "        # Value of the criterion and the gradient \n",
    "        criterion, gradient = Oracle (x) \n",
    "\n",
    "        # Convergence test \n",
    "        gradient_norm = norm (gradient) \n",
    "        if gradient_norm <= threshold: \n",
    "            break \n",
    "\n",
    "        # Direction of descent \n",
    "        direction = -gradient\n",
    "        \n",
    "        # Update of variables \n",
    "        x = x + (gradient_step * direction) \n",
    "        \n",
    "        # Evolution of gradient, step, and criterion \n",
    "        gradient_norm_list.append (gradient_norm) \n",
    "        gradient_step_list.append (gradient_step) \n",
    "        criteria_list.append (criterion) \n",
    "    \n",
    "    ##### Results optimization \n",
    "\n",
    "    criteria_opt = criteria \n",
    "    gradient_opt = gradient \n",
    "    x_opt = x \n",
    "    time_cpu = process_time () - time_start \n",
    "    \n",
    "    print () \n",
    "    print ('Iteration:', k) \n",
    "    print ('CPU time:', time_cpu) \n",
    "    print ('Optimal criterion:', criterion_opt) \n",
    "    print ('Gradient norm:', norm (gradient_opt)) \n",
    "    \n",
    "    # Visualization of convergence\n",
    "    Visualg (gradient_norm_list, gradient_step_list, criterere_list) \n",
    "    \n",
    "    return criterere_opt, gradient_opt, x_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
